---
title: "Simple Linear Regression"
author: "Created by Dr. Mine Dogucu and Presented by Dr. Jessica Jaynes"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css"]
    lib_dir: libs
    seal: false
    nature:
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"


---

class: title-slide

<br>
<br>
.right-panel[ 

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`
]

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(gridExtra)
options(scipen = 999)
```

---

class: middle

## Goals


- Linear Regression with Numeric Predictors
- Conditions for Linear Regression

---
class: middle

# Association
Two variables measured on the same cases are _associated_ if knowing the value of one of the variables tells you something that you would not otherwise know about the value of the other variable.
 
---
class: middle
# Explanatory and Response Variables
Is your purpose simply to explore the nature of the relationship, or do you hope to show that one of the variables can explain variation in the other? 

__Response variable__: measures an outcome of a study (independent variable)
__Explanatory variable__: explains or causes changes in the response variable (dependent variable)

---
class: middle
# Scatterplot
- Shows the relationship between two quantitative variables measured on the same individuals.
- The values of one variable appear on the horizontal axis, and the values of the other variable appear on the vertical axis. 
- Each individual corresponds to one point on the graph.

You can describe the overall pattern of a scatterplot by the direction, form, and strength of the relationship. An important kind of departure is an outlier, an individual value that falls outside the overall pattern of the relationship.


---
class: middle
# Measuring Linear Association
A scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. Linear relationships are important because a straight line is a simple pattern that is quite common.

Our eyes are not always good judges of how strong a relationship is. Therefore, we use a numerical measure to supplement our scatterplot and help us interpret the strength of the linear relationship.

The __correlation__ (r) measures the direction and strength of the linear relationship between two quantitative variables.

---
class: middle
# Properties of Correlation

- r is always a number between –1 and 1.
- r > 0 indicates a positive association.
- r < 0 indicates a negative association.
- Values of r near 0 indicate a very weak linear relationship.
- The strength of the linear relationship increases as r moves away from 0 toward –1 or 1.
- The extreme values r = –1 and r = 1 occur only in the case of a perfect linear relationship. 
- r has no units and does not change when we change the units of 	measurement of x, y, or both.
- Correlation makes no distinction between explanatory and response variables.

---
class: middle
# Cautions with Correlation :
Correlation requires that both variables be quantitative.
Correlation does not describe curved relationships between variables, no matter how strong the relationship is.
The correlation r is not resistant; it can be strongly affected by a few outlying observations.
Correlation is not a complete summary of two-variable data.

---

class: middle

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(openintro)
library(broom)
theme_set(theme_gray(base_size = 18))
```

#### Data `babies` in `openintro` package

```{r echo = FALSE}


glimpse(babies)

```

---

##  Baby Weights

.pull-left[
```{r eval = FALSE, fig.align='center', message = FALSE, warning = FALSE}


ggplot(babies, 
       aes(x = gestation, y = bwt)) +
  geom_point()

```

]

.pull-right[

```{r echo = FALSE, fig.align='center', message = FALSE, fig.height= 6,warning = FALSE}


ggplot(babies, 
       aes(x = gestation, y = bwt)) +
  geom_point()

```

]

---
# Inference for Linear Regression
When a scatterplot shows a linear relationship, we can use the least-squares line fitted to the data to predict y for a given value of x. If the data are a random sample from a larger population, we need statistical inference to answer questions like these:
- Is there really a linear relationship between x and y in the population, or could the pattern we see in the scatterplot plausibly happen just by chance?
- What is the slope (rate of change) that relates y to x in the population, including a margin of error for our estimate of the slope?


---

##  Baby Weights

.pull-left[
```{r eval = FALSE, fig.align='center', message = FALSE, warning = FALSE}
ggplot(babies,
         aes(x = gestation, y = bwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 

```

`lm` stands for linear model  
`se` stands for standard error
]

.pull-right[

```{r echo = FALSE, fig.align='center', message = FALSE, fig.height= 6,warning = FALSE}


ggplot(babies,
         aes(x = gestation, y = bwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 

```

]



---

class: middle

<div align = "center">

| y | Response    | Birth weight | Numeric |
|---|-------------|-----------------|---------|
| x | Explanatory | Gestation           | Numeric |


---

## Linear Equations Review


.pull-left[

Recall from your previous math classes

$y = mx + b$

where $m$ is the slope and $b$ is the y-intercept

e.g. $y = 2x -1$
]

--



.pull-right[
```{r echo = FALSE, fig.height = 5, message = FALSE}
x <- c(0, 1, 2, 3, 4, 5)
y <- c(-1, 1, 3, 5, 7, 9)

as.data.frame(x = x, y = y) %>% 
  ggplot() +
  aes(x = x, y = y) +
  geom_point() +
  scale_y_continuous(breaks = c(-1, 1, 3, 5, 7, 9)) +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4, 5)) +
  geom_smooth(method = "lm", se = FALSE)

```

Notice anything different between baby weights plot and this one?
]

---

class: middle 

.pull-left[

**Math** class

$y = b + mx$

$b$ is y-intercept  
$m$ is slope  
]


.pull-left[

**Stats** class

$y_i = \beta_0 +\beta_1x_i + \epsilon_i$

$\beta_0$ is y-intercept  
$\beta_1$ is slope  
$\epsilon_i$ is error/residual  
$i = 1, 2, ...n$ identifier for each point
]

---

class: middle


```{r}
model_g <- lm(bwt ~ gestation, data = babies)

```

`lm` stands for linear model. We are fitting a linear regression model. Note that the variables are entered in y ~ x order.

---
class: middle
```{r}
summary(model_g)
```



---

class: middle

```{r}
broom::tidy(model_g)
```


--

$\hat {y}_i = b_0 + b_1 x_i$

$\hat {\text{bwt}_i} = b_0 + b_1 \text{ gestation}_i$

$\hat {\text{bwt}_i} = -10.1 + 0.464\text{ gestation}_i$

---

class: middle

## Expected bwt for a baby with 300 days of gestation

$\hat {\text{bwt}_i} = -10.1 + 0.464\text{ gestation}_i$

$\hat {\text{bwt}} = -10.1 + 0.464 \times 300$

$\hat {\text{bwt}} =$ `r -10.1 + 0.464*300`


For a baby with 300 days of gestation the expected birth weight is `r -10.1 + 0.464*300` ounces.

---

## Interpretation of estimates

.pull-left[
```{r echo = FALSE, fig.align='center', message=FALSE, warning = FALSE, fig.height = 4}
babies %>% 
  ggplot() +
  aes(x = gestation, y = bwt) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 
    
```

$b_1 = 0.464$ which means for one unit(day) increase in gestation period the expected increase in birth weight is 0.464 ounces.

]

--

.pull-right[
```{r echo = FALSE, fig.align='center', message=FALSE, warning = FALSE, fig.height = 4}
babies %>% 
  ggplot() +
  aes(x = gestation, y = bwt) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlim(0, 360) +
  ylim(-10, 180) +
  geom_abline(slope = 0.459, intercept = -8.76)
  
  
```

$b_0 = -10.1$ which means for gestation period of 0 days the expected birth weight is -10.1 ounces!!!!!!!!
(does NOT make sense)
]

---

class: middle

## Extrapolation

- There is no such thing as 0 days of gestation.

--

- Birth weight cannot possibly be -10.1 ounces.

--

- Extrapolation happens when we use a model outside the range of the x-values that are observed. After all, we cannot really know how the model behaves (e.g. may be non-linear) outside of the scope of what we have observed. 

---

## Baby number 148

.pull-left[

```{r}
babies %>% 
  filter(case == 148) %>% 
  select(bwt, gestation)

```

]

.pull-right[

```{r echo = FALSE, message = FALSE, fig.height=5, warning = FALSE}

baby_148 <- subset(babies, case == 148)

babies %>% 
  ggplot() +
  aes(x = gestation, y = bwt) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = baby_148, color = "red")
```


]

---

class: middle



## Baby #148

.pull-left[

**Expected**

$\hat y_{148} = b_0 +b_1x_{148}$

$\hat y_{148} = -10.1 + 0.464\times300$

$\hat y_{148}$ = `r -10.1 + 0.464*300`


]

.pull-left[

**Observed**

$y_{148} =$ 160

]

---

## Residual for `i = 148`

.pull-left[

```{r echo = FALSE, fig.align='center', message=FALSE, warning = FALSE, fig.height = 4}
babies %>% 
  ggplot() +
  aes(x = gestation, y = bwt) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = baby_148, color = "red") +
  geom_segment(x = 300, xend = 300, y = 128.94, yend = 160, color = "red")
```



]

.pull-right[

$y_{148} = 160$


<hr>

$\hat y_{148}$ = `r -10.1 + 0.464*300`


<hr>

$e_{148} = y_{148} - \hat y_{148}$

$e_{148} =$ `r 160 -(-10.1 + 0.464*300)`


]

---

## Least Squares Regression 

The goal is to minimize 

$$e_1^2 + e_2^2 + ... + e_n^2$$

--

which can be rewritten as 

$$\sum_{i = 1}^n e_i^2$$

---

## Conditions for Least Squares Regression

- Linearity

- Normality of Residuals 

- Constant Variance

- Independence


---

.pull-left[

.center[**Linear**]
```{r  echo = FALSE, message = FALSE}
set.seed(84735)
x <- seq(-2, 2, by = 0.01)
y <- 4*x + 5 + rnorm(length(x), 0 , 1.5)

data_good <- data.frame(x = x, y = y) %>%   sample_n(50)

data_good %>% 
  ggplot() +
  aes(x = x, y = y) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

]

.pull-right[

.center[**Non-linear**]
```{r  echo = FALSE, message = FALSE}
set.seed(84735)
x <- seq(-2, 2, by = 0.01)
y <- 3*x^2 + x + 5 + rnorm(length(x), 0 , 2)

data_bad<- data.frame(x = x, y = y) %>% sample_n(50)

data_bad %>% 
  ggplot() +
  aes(x = x, y = y) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  stat_function(fun = function(x)3*x^2 + x + 5 )
```

]

---

.pull-left[

.center[**Nearly normal**]

```{r echo = FALSE, message = FALSE}
model_good <- lm(y ~ x, data = data_good)

data_good <- 
  data_good %>%
  sample_n(50) %>% 
  modelr::add_residuals(model_good) 

data_good %>% 
  ggplot(aes(x = resid)) +
  geom_density()

```

]

.pull-right[

.center[**Not normal**]


```{r echo = FALSE, message = FALSE}
model_bad <- lm(y ~ x, data = data_bad)

data_bad <- 
  data_bad %>% 
  modelr::add_residuals(model_bad) 

data_bad %>% 
  ggplot(aes(x = resid)) +
  geom_density()

```

]

---

.pull-left[

.center[**Constant Variance**]

```{r echo = FALSE, message = FALSE}

data_good %>% 
  ggplot(aes(x = x, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0)

```

]

.pull-right[

.center[**Non-constant variance**]


```{r echo = FALSE, message = FALSE}


data_bad %>% 
  ggplot(aes(x = x, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0)

```

]

---

class: middle

## Independence

Harder to check because we need to know how the data were collected.

--

In the description of the dataset it says _[a study]considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area._

--

It is possible that babies born in the same hospital may have similar birth weight. 

--

Correlated data examples: patients within hospitals, students within schools, people within neighborhoods, time-series data. 

---

## Conditions for Least Squares Regression
```{r}
plot(model_g, 1:2)
```




---

class: middle

### Inference: Confidence Interval (theoretical)

```{r}
confint(model_g)
```

Note that the 95% confidence interval for the slope does not contain zero and all the values in the interval are positive indicating a significant positive relationship between gestation and birth weight.


---
# Coefficient of Determination: Connection between correlation and regression

The square of the correlation, $R^2$, is the fraction of the variation in values of y that is explained by the least-squares regression of y on x. 
$R^2$ is called the coefficient of determination.


---
# Correlation Does Not Imply Causation
- Even when direct causation is present, it may not be the whole explanation for a correlation.
- Always worry about lurking variables and try to anticipate lurking variables and measure them
- Do an experiment in which we change x and keep lurking variables under control.  

